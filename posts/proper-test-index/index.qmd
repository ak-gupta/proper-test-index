---
title: "The Proper Test Index"
author: "Akshay Gupta"
date: "2025-06-26"
date-modified: "2025-06-29"
categories: [golf]
jupyter: python3
execute:
    echo: false
subtitle: What does Big Randy's metric tell us about the PGA Tour?
---

```{python}
from pathlib import Path

import polars as pl
from lets_plot import *

from proper_test_index.schemas import ScoreObject, to_schema

LetsPlot.setup_html()

def article_theme():
    return theme(
        plot_title=element_text(family="Arial", face="bold", size=33),
        plot_subtitle=element_text(family="Arial", size=26),
        legend_position="top",
        panel_grid_minor="blank",
        panel_grid_major_x="blank",
    )
```

In No Laying Up's coverage of the 125th U.S. Open at Oakmont, the Big Man himself
introduced a metric that has lodged itself in my brain: the Proper Test Index (PTI).
It's defined as

$$
PTI = \frac{\text{Number of rounds with a score >= 80}}{\text{Number of rounds with a score of < 70}}
$$

He ran through some of the numbers across past U.S. Opens, but what does it look like when we
extend the metric to the PGA Tour? Using Data Golf's API, I pulled historical scoring data
and calculated the event-level PTI:

```{python}
pti = (
    pl.read_csv("../../pti.csv")
    .with_columns([pl.col("year").cast(pl.String)])
)

easiest_event = (
    pti.filter(
        pl.col("over_80") == pl.lit(0)
    )
    .sort("sub_70", descending=True)
    .head(n=1)
)

plot_def_ = (
    geom_point(
        aes(x="over_80", y="sub_70", color="major_championship", size="pti"),
        alpha=0.2,
        tooltips=(
            layer_tooltips()
            .line("@year @event_name")
            .line("Venue|@course_name")
            .line("PTI|@pti")
        )
    ) +
    ggsize(800, 600) +
    article_theme()
)
initial_plot = (
    ggplot(pti) +
    plot_def_ +
    labs(
        title="Rounds over 80 against sub-70 rounds",
        subtitle="All available events in Data Golf's archives",
        x="# of rounds with a score of 80+",
        y="# of rounds with a score of <70",
        color="Is it a major championship?",
        size="Proper Test Index"
    )
)
initial_plot
```

A few things stood out to me.

* The pros are *really* good. Across the entire dataset, only `{python} round((pti["over_80"].sum() / pti["total_rounds"].sum()) * 100, 1)`% of rounds have a score of 80 or over.
  That's compared to a healthy `{python} round((pti["sub_70"].sum() / pti["total_rounds"].sum()) * 100, 1)`% of rounds falling in the 60s (or lower!).
* There are `{python} pti.filter(pl.col("over_80") == pl.lit(0)).shape[0]` events that had **0 rounds of 80 or over**.
* What were we doing at the `{python} easiest_event["year"][0]` `{python} easiest_event["event_name"][0]`?
  `{python} easiest_event["sub_70"][0]` rounds below 70 and 0 above 80. Not Proper.

Now when it comes to majors...

```{python}
# Let's get the easiest and hardest majors
majors = (
    pti.filter(pl.col("major_championship"))
    .sort("pti", descending=True)
)
easiest_major = majors.tail(n=1)
hardest_major = majors.head(n=1)

(
    ggplot(majors) +
    geom_point(
        aes(x="over_80", y="sub_70", size="pti"),
        alpha=0.5,
        tooltips=(
            layer_tooltips()
            .line("@year @event_name")
            .line("Venue|@course_name")
            .line("PTI|@pti")
        )
    ) +
    ggsize(800, 600) +
    article_theme() +
    labs(
        title="Rounds over 80 against sub-70 rounds",
        subtitle="Major championships in Data Golf's archives",
        x="Count of rounds with a score of 80+",
        y="Count of rounds with a score of <70",
        size="Proper Test Index"
    )
)
```

As much as I like Brooks Koepka, the `{python} easiest_major["year"][0]` `{python} easiest_major["event_name"][0]`
at `{python} easiest_major["course_name"][0]` wasn't particularly *proper*. But good news for Big! Phil the Thrill's
unbelievable performance in the `{python} hardest_major["year"][0]` `{python} hardest_major["event_name"][0]`
at `{python} hardest_major["course_name"][0]` was undeniably a Proper Victory. Soly's right, we _somehow_ don't talk
about that win enough.


## Sabermetrics can help us identify *Proper Venues*

PTI tells us something about the balance of extreme rounds at the event level. What if we adopt
Baseball's *park factor* to figure out if a low round really matters?

$$
\text{Course Factor} = \frac{\frac{\text{Total scores >= 80 at the course}}{\text{Total scores < 70 at the course}}}{\frac{\text{Total scores >= 80 at all other courses}}{\text{Total scores < 70 at all other courses}}}
$$

The larger the *course factor*, the more impressive a 65 is! Let's plot out the *course factor* across the available
venues.

```{python}
course_factor = pl.read_csv("../../course_factor.csv")
hardest_course = course_factor.sort("course_factor", descending=True).head(n=1)
easiest_course = (
    course_factor.sort("course_factor", descending=False)
    .filter(pl.col("course_factor") > 0)
    .head(n=1)
)

course_factor_plot = (
    ggplot(course_factor) +
    geom_point(
        aes(x="course_factor", y="scoring_average"),
        tooltips=(
            layer_tooltips()
            .line("@course_name")
            .line("Total 80+ rounds|@total_over_80")
            .line("Total <70 rounds|@total_sub_70")
        )
    ) +
    labs(
        title='Venue scoring average by "course factor"',
        x="Course Factor",
        y="Scoring average"
    ) +
    ggsize(800, 600) +
    article_theme()
)
course_factor_plot
```

Wow. _Course Factor_ has some range! According to this metric, An above average round at `{python} hardest_course["course_name"][0]`
is worth `{python} int(hardest_course["course_factor"][0] / easiest_course["course_factor"][0])` times more than a
similarly above average round at `{python} easiest_course["course_name"][0]`. Sure.

We're seeing the effects of _rare_ events; only `{python} round((pti["over_80"].sum() / pti["total_rounds"].sum()) * 100, 1)`% of rounds come in over 80, so if `{python} round((hardest_course["total_over_80"][0] / hardest_course["total_rounds"][0]) * 100, 1)`%
of rounds at `{python} hardest_course["course_name"][0]` are over 80 (and only `{python} round((hardest_course["total_sub_70"][0] / hardest_course["total_rounds"][0]) * 100, 1)`%
below 70), we will see extreme values.

What we need is a monotonic transformation that can _compress_ the scale a bit. Using $\log$, we can get something
a bit more readable.

```{python}
(
    course_factor_plot +
    scale_x_log10() +
    geom_label(
        x=60,
        y=68,
        hjust=0,
        label="Yikes... maybe TC\nwas right about\nTPC Toronto",
        label_size=0,
        family="Helvetica"
    ) +
    geom_curve(
        x=60,
        y=68,
        xend=40,
        yend=68.5,
        curvature=-0.2,
        arrow=arrow()
    )
)
```

This chart is more legible. Due to the strict scarcity of rounds in the 80s, we'll be using a logarithmic
transformation of course factor ($\text{Course Factor}^{*}$) to convey the relative difficulty of each venue.

Now comes the real question, however: **Did Big Cook?**

Looking at the distribution of scores across each venue,

```{python}
scoring_data = (
    pl.read_parquet(
        [fpath for fpath in (Path.cwd() / ".." / ".." / "data").glob("**/*-scoring-data.parquet")],
        schema=to_schema(ScoreObject)
    )
    .join(
        (
            course_factor
            .select([pl.col("course_factor").rank("ordinal", descending=True).alias("rank"), "course_factor", "course_num"])
        ),
        how="left",
        on="course_num"
    )
    .with_columns(
        count=(pl.len().over(["score", "course_num"]) / pl.len().over(["course_num"])) * pl.lit(100.0),
        log_course_factor=(pl.lit(1.0) + pl.col("course_factor")).log10()
    )
)

(
    ggplot(scoring_data) +
    geom_area_ridges(
        aes(x="score", y="rank", height="count", fill="log_course_factor"),
        stat="identity",
        scale=0.5,
        sampling=sampling_pick(scoring_data.shape[0]),
        quantiles=[0.5],
        quantile_lines=True,
        tooltips=(
            layer_tooltips()
            .format("@count", ".2f")
            .format("@log_course_factor", ".1f")
            .line("@course_name")
            .line("Course factor* (rank)|@log_course_factor (@rank)")
            .line("Score|@score")
            .line("% of rounds|@count")
        )
    ) +
    ggsize(800, 800) +
    scale_fill_gradient2() +
    labs(
        title="Distribution of scores by venue",
        x="Score",
        fill="Course Factor*"
    ) +
    theme(
        plot_title=element_text(family="Arial", face="bold", size=33),
        axis_text_y=element_blank(),
        axis_title_y="blank",
        panel_grid_minor="blank",
        panel_grid_major="blank",
    )
)
```

I think Big might have cooked here! $\text{Course Factor}^{*}$ seems to correlate not only with average difficulty,
but *variance*. In a proper test, we *want* to see separation between the "Men" and the "Little Boys".

There's certainly more to be said on this topic. However, next up: the *Proper Player Index*. Stay tuned.
